{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4f2d06c9",
      "metadata": {
        "id": "4f2d06c9"
      },
      "source": [
        "# DAY 3 – PROMPT ENGINEERING & LLM BASICS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e57a0306",
      "metadata": {
        "id": "e57a0306"
      },
      "source": [
        "\n",
        "## What is Prompt Engineering?\n",
        "   Prompt Engineering is the process giving effective instructions to the models in order to get desired output.\n",
        "- Prompt engineering = designing effective instructions for LLMs.\n",
        "- Types: instruction, role, zero-shot, few-shot, chain-of-thought.\n",
        "- Good prompts: clear task, context, examples, structured output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "eb474553",
      "metadata": {
        "id": "eb474553"
      },
      "outputs": [],
      "source": [
        "\n",
        "bad_prompt = \"Explain overfitting.\"\n",
        "\n",
        "good_prompt = '''Explain overfitting to a new data science student\n",
        "using simple language and an example.'''\n",
        "\n",
        "best_prompt = '''You are a friendly Data Science teacher.\n",
        "Explain overfitting to a first-year Indian BSc student using:\n",
        "- 1 analogy\n",
        "- 1 simple ML example\n",
        "- Bullet points\n",
        "Keep it under 120 words.'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1351dc",
      "metadata": {
        "id": "1b1351dc"
      },
      "source": [
        "### **Bad Prompt Output:** Overfitting is when a model learns the training data too closely—including noise or random fluctuations—so it performs well on that data but poorly on new, unseen data.\n",
        "\n",
        "### **Good Prompt Output:** Overfitting happens when a model learns the training data too well — not just the real patterns, but also the noise and quirks that won’t appear again in new data.\n",
        "\n",
        "**A simple example:**\n",
        "\n",
        "• Imagine you’re trying to teach a friend to recognize dogs.\n",
        "• You show them 10 pictures. In one picture, the dog is sitting on a red carpet.\n",
        "• If your friend decides “dogs always sit on red carpets,” they’ve learned something too specific.\n",
        "• They’ll do great on those 10 training pictures but fail on new dog photos without red carpets.\n",
        "\n",
        "That’s overfitting: great performance on training data, poor performance on new data, because the model memorized details instead of learning general patterns.\n",
        "\n",
        "### **Best Prompt Output:**- **Analogy:** Overfitting is like memorizing every question from last year’s exam paper. You score well if the same questions repeat, but struggle when the exam has new ones.\n",
        "\n",
        "- **Simple ML example:** If you train a model to predict house prices and it memorizes every tiny detail of the training houses—including unusual quirks—it will predict perfectly on those houses but poorly on new ones.\n",
        "\n",
        "- **Key idea:**  \n",
        "  - Learns noise instead of real patterns  \n",
        "  - High training accuracy, low test accuracy  \n",
        "  - Fixed by simpler models, more data, or regularization\n",
        "\n",
        "### **Observation:** From these examples we can observe that, we can get the best response when we give the best prompt to the model. If we give concisely it simply gave defination when we asked to explain to a data science student in simple language and with an example it has given the good response by clearly explaining what overfitting is with example and in simple language and finally when we gave best prompt by giving all insttructions and context it explained with beautiful example that too with simple words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2a12e2f5",
      "metadata": {
        "id": "2a12e2f5"
      },
      "outputs": [],
      "source": [
        "\n",
        "zero_shot_prompt = '''Classify sentiment: \"Movie was okay but slow.\"'''\n",
        "\n",
        "few_shot_prompt = '''\n",
        "You are a sentiment classifier.\n",
        "Examples:\n",
        "\"I loved it\" -> Positive\n",
        "\"I hated it\" -> Negative\n",
        "\"It was okay\" -> Neutral\n",
        "Classify: \"Movie was okay but slow.\"\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272564ce",
      "metadata": {
        "id": "272564ce"
      },
      "source": [
        "### Zero-shot Output:Sentiment: Neutral / Slightly Negative\n",
        "\n",
        "### Few-shot Output:Neutral\n",
        "\n",
        "### Observation: Since we won't provide examples in zero shot prompt it gave the answer like neutral/slightly Negative whereas in few shot as we provided the examples so it understood the patern well and gave direct answer. So based on our requirement we should decide what kind of prompt makes sense to the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "43d6e432",
      "metadata": {
        "id": "43d6e432"
      },
      "outputs": [],
      "source": [
        "\n",
        "no_cot = \"15 apples, sells 7, buys 4. Final answer only.\"\n",
        "\n",
        "cot = '''Solve step-by-step:\n",
        "15 apples -> sell 7 -> buy 4.\n",
        "Explain steps and give final answer.'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4868dbed",
      "metadata": {
        "id": "4868dbed"
      },
      "source": [
        "### No CoT Output:12\n",
        "\n",
        "### CoT Output:Step-by-step:\n",
        "\n",
        "Start with 15 apples.\n",
        "\n",
        "Sell 7 apples: compute\n",
        "15\n",
        "−\n",
        "7\n",
        "15−7.\n",
        "\n",
        "Ones place: 5 − 7 → borrow 1 ten → (15 − 7) = 8.\n",
        "\n",
        "Result after selling: 8 apples.\n",
        "\n",
        "Buy 4 apples: compute\n",
        "8\n",
        "+\n",
        "4\n",
        "=\n",
        "12\n",
        "8+4=12.\n",
        "\n",
        "Ones: 8 + 4 = 12 → write 2, carry 1 (gives 12 total).\n",
        "\n",
        "Final answer: 12 apples.\n",
        "\n",
        "### Observation: Here both answers are correct since models got updated but for more complex reasoning tasks machine might not give correct answers to the non cot prompts due to not knowing right logic but when we give cot prompt it understands the logic behind the question and gives the accurate answer. Moreover, we can observe that since we asked to give steps it explained the logic step by step so that we can understand how it is solving the question and can ensure whether it is doing in right way or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "6ce17a7e",
      "metadata": {
        "id": "6ce17a7e"
      },
      "outputs": [],
      "source": [
        "role_prompt = \"You are a strict Python interviewer. Ask 5 questions (no answers).\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c645784",
      "metadata": {
        "id": "9c645784"
      },
      "source": [
        "### Output:\n",
        "Here are 5 strict Python interview questions (no answers):\n",
        "\n",
        "Explain the difference between a shallow copy and a deep copy in Python. When would you use each?\n",
        "\n",
        "What are iterators and generators? How does yield work internally?\n",
        "\n",
        "Describe how Python’s GIL (Global Interpreter Lock) affects multithreading. When is multiprocessing preferred?\n",
        "\n",
        "What is the difference between @staticmethod, @classmethod, and instance methods? Give use cases.\n",
        "\n",
        "How does Python handle memory management? Explain reference counting and garbage collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "193c4a33",
      "metadata": {
        "id": "193c4a33"
      },
      "outputs": [],
      "source": [
        "json_prompt = '''Extract name, issue, urgency from:\n",
        "\"Hi, I'm Riya. My phone isn't charging. It's urgent.\"\n",
        "Return JSON only.'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0c313f",
      "metadata": {
        "id": "ab0c313f"
      },
      "source": [
        "### Output (JSON):\n",
        "{\n",
        "  \"name\": \"Riya\",\n",
        "  \"issue\": \"Phone isn't charging\",\n",
        "  \"urgency\": \"Urgent\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c130173e",
      "metadata": {
        "id": "c130173e"
      },
      "source": [
        "\n",
        "# Prompt Engineering Cheatsheet\n",
        "- Be specific\n",
        "- Provide context\n",
        "- Ask for structured output\n",
        "- Use few-shot examples\n",
        "- Use CoT when needed\n",
        "- Use constraints (tone, word limit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "588787f4",
      "metadata": {
        "id": "588787f4"
      },
      "source": [
        "## Final Notes\n",
        "- What I learned: Today, I learned what is Prompt Engineering, their types, tools, techniques and basic of llm like its settings and all.\n",
        "- What to revise: Should revise all the techniques of prompts"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}