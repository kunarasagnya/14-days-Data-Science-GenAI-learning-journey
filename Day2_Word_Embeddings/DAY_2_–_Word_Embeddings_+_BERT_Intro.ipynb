{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk gensim sentence-transformers"
      ],
      "metadata": {
        "id": "XXOuJyhELPao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load the libraries/packages\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from nltk.corpus import movie_reviews\n",
        "from gensim.models import Word2Vec\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "3WzfPdhSLPAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Download NLTK resources\n",
        "nltk.download(\"movie_reviews\")\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "kJrB2J3SLPKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load movie_reviews dataset and prepare sentences\n",
        "# movie_reviews.words(fileid) already gives tokenized words\n",
        "sentences = [list(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()]\n",
        "print(f\"Number of documents: {len(sentences)}\")\n",
        "print(sentences[0][:40])"
      ],
      "metadata": {
        "id": "A4kkoO0BL7pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train a Word2Vec model on movie_reviews\n",
        "# Word2Vec hyperparameters chosen to be small & fast\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,   # embedding size\n",
        "    window=5,          # context window\n",
        "    min_count=3,       # ignore rare words\n",
        "    workers=4,         # number of CPU cores to use\n",
        "    sg=1,              # 1 = skip-gram; 0 = CBOW\n",
        "    epochs=10          # training iterations\n",
        ")\n",
        "\n",
        "print(\"Word2Vec training complete!\")\n",
        "print(\"Vocabulary size:\", len(w2v_model.wv))\n"
      ],
      "metadata": {
        "id": "I91aVeZwMJpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Explore word similarities (Word2Vec)\n",
        "\n",
        "def show_similar_words(word, topn=10):\n",
        "    if word not in w2v_model.wv:\n",
        "        print(f\"'{word}' not in vocabulary.\")\n",
        "        return\n",
        "    print(f\"\\nTop {topn} words similar to '{word}':\")\n",
        "    for sim_word, score in w2v_model.wv.most_similar(word, topn=topn):\n",
        "        print(f\"{sim_word:15s}  ->  {score:.3f}\")\n",
        "\n",
        "show_similar_words(\"good\")\n",
        "show_similar_words(\"bad\",5)\n",
        "show_similar_words(\"movie\",3)\n"
      ],
      "metadata": {
        "id": "ApILwFwaMiMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Cosine similarity between word embeddings\n",
        "\n",
        "def cosine_similarity(word1, word2):\n",
        "    if word1 not in w2v_model.wv or word2 not in w2v_model.wv:\n",
        "        print(f\"One of the words ('{word1}', '{word2}') is not in vocabulary.\")\n",
        "        return None\n",
        "    v1 = w2v_model.wv[word1]\n",
        "    v2 = w2v_model.wv[word2]\n",
        "    sim = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "    return sim\n",
        "\n",
        "pairs = [\n",
        "    (\"good\", \"great\"),\n",
        "    (\"good\", \"bad\"),\n",
        "    (\"good\", \"excellent\"),\n",
        "    (\"bad\", \"terrible\"),\n",
        "    (\"movie\", \"film\"),\n",
        "    (\"movie\", \"dog\")\n",
        "]\n",
        "\n",
        "print(\"\\nCosine similarities between word pairs (Word2Vec):\")\n",
        "for w1, w2 in pairs:\n",
        "    sim = cosine_similarity(w1, w2)\n",
        "    if sim is not None:\n",
        "        print(f\"{w1:8s} vs {w2:9s} -> {sim:.3f}\")\n"
      ],
      "metadata": {
        "id": "J7yt0vwxM1SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Visualize a few word embeddings using PCA\n",
        "\n",
        "words_to_plot = [\"good\", \"great\", \"bad\", \"awful\", \"movie\", \"film\", \"dog\", \"cat\"]\n",
        "existing_words = [w for w in words_to_plot if w in w2v_model.wv]\n",
        "\n",
        "vectors = np.array([w2v_model.wv[w] for w in existing_words])\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(vectors)\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.scatter(reduced[:, 0], reduced[:, 1])\n",
        "\n",
        "for i, word in enumerate(existing_words):\n",
        "    plt.annotate(word, (reduced[i, 0] + 0.01, reduced[i, 1] + 0.01))\n",
        "\n",
        "plt.title(\"PCA projection of Word2Vec embeddings\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2cJUn4CgNEG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Sentence Embeddings using Sentence-BERT (SBERT)\n",
        "\n",
        "# Load a lightweight SBERT model\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"\\nSBERT model loaded.\")\n",
        "\n",
        "sentences = [\n",
        "    \"I love this movie, it was amazing!\",\n",
        "    \"This film was terrible and boring.\",\n",
        "    \"The movie was okay, not great but not bad.\",\n",
        "    \"I really enjoyed this film, it was fantastic.\",\n",
        "    \"The plot was dull and the acting was bad.\"\n",
        "]\n",
        "\n",
        "sentence_embeddings = sbert_model.encode(sentences, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "l92wg2cjNNX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Compute pairwise sentence similarity\n",
        "\n",
        "similarity_matrix = util.cos_sim(sentence_embeddings, sentence_embeddings)\n",
        "similarity_df = pd.DataFrame(\n",
        "    similarity_matrix.cpu().numpy(),\n",
        "    index=[f\"S{i}\" for i in range(len(sentences))],\n",
        "    columns=[f\"S{i}\" for i in range(len(sentences))]\n",
        ")\n",
        "\n",
        "print(\"\\nSentences:\")\n",
        "for i, s in enumerate(sentences):\n",
        "    print(f\"S{i}: {s}\")\n",
        "\n",
        "print(\"\\nCosine similarity matrix between sentences (SBERT):\")\n",
        "display(similarity_df.round(3))"
      ],
      "metadata": {
        "id": "Ddz6USFINUdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Show most similar sentence for each input\n",
        "\n",
        "def show_most_similar_sentence(sent_list):\n",
        "    emb = sbert_model.encode(sent_list, convert_to_tensor=True)\n",
        "    sim = util.cos_sim(emb, sentence_embeddings)  # compare to our base sentences\n",
        "\n",
        "    for i, query in enumerate(sent_list):\n",
        "        sims = sim[i].cpu().numpy()\n",
        "        best_idx = int(np.argmax(sims))\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        print(f\"Most similar: {sentences[best_idx]}\")\n",
        "        print(f\"Similarity score: {sims[best_idx]:.3f}\")\n",
        "\n",
        "test_queries = [\n",
        "    \"I hated this movie, it was so bad.\",\n",
        "    \"One of the best films I have ever seen.\",\n",
        "    \"It was an average film.\"\n",
        "]\n",
        "\n",
        "show_most_similar_sentence(test_queries)"
      ],
      "metadata": {
        "id": "3Z40XhIHNmZ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}